{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1898bc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09a8a54a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\infosys\\\\notebook'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "388efaf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\infosys'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c4cbe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class faiss_data:\n",
    "    index_path: Path\n",
    "    metadata_path: Path\n",
    "    top_k: int\n",
    "\n",
    "@dataclass\n",
    "class llmconfig:\n",
    "    provider: str\n",
    "    model: str\n",
    "    temperature: float\n",
    "    max_tokens: int\n",
    "\n",
    "@dataclass\n",
    "class neo4j_config:\n",
    "    uri: str\n",
    "    username: str\n",
    "    password: str\n",
    "\n",
    "@dataclass\n",
    "class Ragpipelineconfig:\n",
    "    input_json: Path\n",
    "    faiss: faiss_data\n",
    "    neo4j: neo4j_config\n",
    "    llm: llmconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d609e473",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.knowledge_graph.utils.common import read_yaml\n",
    "from src.knowledge_graph.constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f60b493",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self,config_path = CONFIG_FILE_PATH):\n",
    "        self.config = read_yaml(config_path)\n",
    "    \n",
    "    def get_rag_pipeline_config(self)->Ragpipelineconfig:\n",
    "        config = self.config.rag\n",
    "\n",
    "        return Ragpipelineconfig(\n",
    "            input_json = config.input_json,\n",
    "            faiss = faiss_data(index_path = config.faiss.index_path,\n",
    "                        metadata_path = config.faiss.metadata_path,\n",
    "                        top_k = config.faiss.top_k),\n",
    "            neo4j = neo4j_config(uri = config.neo4j.uri,\n",
    "                        username = config.neo4j.username,\n",
    "                        password = config.neo4j.password),\n",
    "            llm = llmconfig(provider = config.llm.provider,\n",
    "                        model = config.llm.model,\n",
    "                        temperature = config.llm.temperature,\n",
    "                        max_tokens = config.llm.max_tokens)  \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7318b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import faiss\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "from langchain_core.documents import Document\n",
    "from neo4j import GraphDatabase\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from typing import List, Any\n",
    "import spacy\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "class HybridRetriever(BaseRetriever):\n",
    "    \"\"\"\n",
    "    Same Retriever logic as before (Vectors + Graph)\n",
    "    \"\"\"\n",
    "    vector_index: Any\n",
    "    vector_metadata: List[dict]\n",
    "    embedder: Any\n",
    "    graph: Any\n",
    "    nlp: Any\n",
    "    top_k_vector: int = 5\n",
    "    top_k_graph: int = 5\n",
    "\n",
    "    def _get_relevant_documents(\n",
    "        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n",
    "    ) -> List[Document]:\n",
    "        \n",
    "        # 1. Vector Search\n",
    "        query_vector = self.embedder.encode([query])\n",
    "        distances, indices = self.vector_index.search(query_vector, self.top_k_vector)\n",
    "        \n",
    "        docs = []\n",
    "        for idx in indices[0]:\n",
    "            if idx < len(self.vector_metadata):\n",
    "                meta = self.vector_metadata[idx]\n",
    "                content = f\"[Source: {meta.get('source_name', 'Unknown')}] {meta.get('text', '')}\"\n",
    "                docs.append(Document(\n",
    "                    page_content=content,\n",
    "                    metadata={\"type\": \"vector\", \"source\": meta.get('source_name')}\n",
    "                ))\n",
    "\n",
    "        # 2. Graph Search (Fixed for neo4j.Driver)\n",
    "        spacy_doc = self.nlp(query)\n",
    "        entities = [ent.text for ent in spacy_doc.ents]\n",
    "        \n",
    "        if entities:\n",
    "            # Open a session properly using the driver\n",
    "            try:\n",
    "                with self.graph.session() as session:\n",
    "                    for entity in entities:\n",
    "                        # Fuzzy match entity names\n",
    "                        cypher = \"\"\"\n",
    "                        MATCH (n:Entity)-[r]-(m:Entity)\n",
    "                        WHERE toLower(n.name) CONTAINS toLower($name)\n",
    "                        RETURN n.name, type(r) AS rel, m.name\n",
    "                        LIMIT $limit\n",
    "                        \"\"\"\n",
    "                        # Use session.run with parameters (safer than f-strings)\n",
    "                        result = session.run(cypher, name=entity, limit=self.top_k_graph)\n",
    "                        \n",
    "                        for record in result:\n",
    "                            fact = f\"{record['n.name']} --[{record['rel']}]--> {record['m.name']}\"\n",
    "                            docs.append(Document(\n",
    "                                page_content=fact,\n",
    "                                metadata={\"type\": \"graph\", \"entity\": entity}\n",
    "                            ))\n",
    "            except Exception as e:\n",
    "                print(f\"Graph query error: {e}\")\n",
    "        \n",
    "        return docs\n",
    "\n",
    "def get_rag_chain():\n",
    "    # 1. Load Config\n",
    "    config = ConfigurationManager().get_rag_pipeline_config()\n",
    "    \n",
    "    # 2. Load Resources (Embeddings, FAISS, Graph)\n",
    "    # Note: We still use SentenceTransformer for embeddings locally to match your FAISS index\n",
    "    embedder = SentenceTransformer('all-MiniLM-L6-v2') \n",
    "    \n",
    "    index = faiss.read_index(config.faiss.index_path)\n",
    "    with open(config.faiss.metadata_path, \"r\") as f:\n",
    "        metadata = json.load(f)\n",
    "        \n",
    "    graph = GraphDatabase.driver(\n",
    "        config.neo4j.uri,\n",
    "        auth=(config.neo4j.username, config.neo4j.password)\n",
    "    )\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # 3. Initialize Retriever\n",
    "    retriever = HybridRetriever(\n",
    "        vector_index=index,\n",
    "        vector_metadata=metadata,\n",
    "        embedder=embedder,\n",
    "        graph=graph,\n",
    "        nlp=nlp,\n",
    "        top_k_vector= config.faiss.top_k,\n",
    "        top_k_graph= 5\n",
    "    )\n",
    "\n",
    "    llm = ChatGroq(\n",
    "        model=config.llm.model,\n",
    "        groq_api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    "        temperature=config.llm.temperature,\n",
    "        max_tokens=config.llm.max_tokens\n",
    "        )\n",
    "\n",
    "    # 5. Prompt & Chain\n",
    "    template = \"\"\"You are a helpful assistant. You firstly greet the user with HI .Then, if you get any context\n",
    "    use that to provide answer to the question with full confidence. If you do not have any context,provide an overview of the \n",
    "    question based on your general knowledge and end with \"As an AI language model, I do not have the information in my Database\".\n",
    "    context given below  \n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdb7f7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-31 01:06:06,225: INFO: common: YAML file: config\\config.yaml loaded successfully]\n",
      "[2026-01-31 01:06:06,333: INFO: SentenceTransformer: Use pytorch device_name: cpu]\n",
      "[2026-01-31 01:06:06,335: INFO: SentenceTransformer: Load pretrained SentenceTransformer: all-MiniLM-L6-v2]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "895d60998e3647c6a61b8201f65201d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-31 01:06:28,757: INFO: _client: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: HI and how can I help you today?\n",
      "\n",
      "Based on the context you provided, it appears that ShopCart is a company that sends emails to its customers or teams. The emails are from different teams within ShopCart, such as Support, Orders, Finance, and Security. This suggests that ShopCart is an e-commerce platform or a company that provides online shopping services.\n",
      "\n",
      "From the emails, it seems that ShopCart has different teams that handle various aspects of the business, including customer support, orders, finance, and security. This implies that ShopCart is a company that operates online and has a structured organizational setup.\n",
      "\n",
      "Based on this information, I can confidently say that ShopCart is an e-commerce platform or a company that provides online shopping services.\n"
     ]
    }
   ],
   "source": [
    "# 1. Define your question\n",
    "question = \"What is Shopcart?\"\n",
    "\n",
    "# 2. Load and run the chain\n",
    "chain = get_rag_chain()\n",
    "response = chain.invoke(question)\n",
    "\n",
    "# 3. Print the result\n",
    "print(f\"Answer: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc68419",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
