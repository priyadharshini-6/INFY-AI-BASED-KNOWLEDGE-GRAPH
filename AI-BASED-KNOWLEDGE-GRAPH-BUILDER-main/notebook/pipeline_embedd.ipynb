{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9cd2fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6b3e254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\infosys\\\\notebook'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bc125b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\infosys'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c854e3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class ChunkingConfig:\n",
    "    chunk_size: int\n",
    "    chunk_overlap: int\n",
    "\n",
    "@dataclass\n",
    "class EmbeddingModelConfig:\n",
    "    name: str\n",
    "    embedding_dim: int\n",
    "\n",
    "@dataclass\n",
    "class VectorStoreConfig:\n",
    "    type: str\n",
    "    index_type: str\n",
    "    index_path: Path\n",
    "    metadata_path: Path\n",
    "\n",
    "@dataclass\n",
    "class EmbeddingPipelineConfig:\n",
    "    input_json: Path\n",
    "    chunking: ChunkingConfig\n",
    "    embedding_model: EmbeddingModelConfig\n",
    "    vector_store: VectorStoreConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8700bc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.knowledge_graph.utils.common import read_yaml\n",
    "from src.knowledge_graph.constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4781e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "\n",
    "    def __init__(self, config_path=CONFIG_FILE_PATH):\n",
    "            self.config = read_yaml(config_path)\n",
    "\n",
    "    def get_pipeline_embedd_config(self) -> EmbeddingPipelineConfig:\n",
    "        config = self.config.pipeline_embedd\n",
    "\n",
    "        return EmbeddingPipelineConfig(\n",
    "            input_json=config.input_json,\n",
    "            chunking=ChunkingConfig(chunk_size=config.chunking.chunk_size,\n",
    "                                    chunk_overlap=config.chunking.chunk_overlap),\n",
    "            embedding_model=EmbeddingModelConfig(name = config.embedding_model.name,\n",
    "                                                 embedding_dim=config.embedding_model.embedding_dim),\n",
    "            vector_store=VectorStoreConfig(type = config.vector_store.type,\n",
    "                                           index_type=config.vector_store.index_type,\n",
    "                                           index_path = config.vector_store.index_path,\n",
    "                                           metadata_path= config.vector_store.metadata_path)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b215a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-19 19:02:17,061: INFO: loader: Loading faiss with AVX2 support.]\n",
      "[2026-01-19 19:02:17,842: INFO: loader: Successfully loaded faiss with AVX2 support.]\n",
      "d:\\infosys\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from src.knowledge_graph.utils.common import read_json, write_json\n",
    "from src.knowledge_graph.logger.logging import logger\n",
    "from src.knowledge_graph.exception.exception import KGException\n",
    "import sys\n",
    "\n",
    "class DataEmbedding:\n",
    "    \"\"\"\n",
    "    Milestone-3: Data Embedding Pipeline\n",
    "    \n",
    "    Optimized for RAG:\n",
    "    - Preserves text content in metadata (Essential for retrieval).\n",
    "    - Uses GPU acceleration if available.\n",
    "    - Implements batched processing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        try:\n",
    "            self.config = config\n",
    "            \n",
    "            # 1. Load Data\n",
    "            self.documents = read_json(self.config.input_json)\n",
    "            \n",
    "            # 2. Setup Device (GPU/CPU)\n",
    "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            logger.info(f\"Embedding Model will run on: {self.device.upper()}\")\n",
    "\n",
    "            # 3. Load Model\n",
    "            self.model = SentenceTransformer(\n",
    "                self.config.embedding_model.name,\n",
    "                device=self.device\n",
    "            )\n",
    "\n",
    "            # 4. Setup Text Splitter\n",
    "            self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=self.config.chunking.chunk_size,\n",
    "                chunk_overlap=self.config.chunking.chunk_overlap,\n",
    "                length_function=len,\n",
    "                is_separator_regex=False,\n",
    "            )\n",
    "\n",
    "            # 5. Output Paths\n",
    "            self.index_path = self.config.vector_store.index_path\n",
    "            self.metadata_path = self.config.vector_store.metadata_path\n",
    "\n",
    "            # Runtime Storage\n",
    "            self.text_chunks = []\n",
    "            self.metadata = []\n",
    "\n",
    "        except Exception as e:\n",
    "            raise KGException(e, sys)\n",
    "\n",
    "    def process_chunks(self):\n",
    "        \"\"\"\n",
    "        Step 1: Chunking\n",
    "        Splits text and prepares metadata.\n",
    "        CRITICAL: We must store the actual text in metadata for RAG retrieval.\n",
    "        \"\"\"\n",
    "        logger.info(\"Starting text chunking...\")\n",
    "        \n",
    "        try:\n",
    "            global_chunk_id = 0\n",
    "            \n",
    "            for doc in self.documents:\n",
    "                # Handle potentially missing text\n",
    "                raw_text = doc.get(\"text\", \"\")\n",
    "                if not raw_text:\n",
    "                    continue\n",
    "                    \n",
    "                chunks = self.text_splitter.split_text(raw_text)\n",
    "\n",
    "                for chunk in chunks:\n",
    "                    self.text_chunks.append(chunk)\n",
    "                    \n",
    "                    # Store rich metadata including the text itself\n",
    "                    self.metadata.append({\n",
    "                        \"id\": global_chunk_id,\n",
    "                        \"document_id\": doc.get(\"id\"),\n",
    "                        \"source_name\": doc.get(\"source_name\"),\n",
    "                        \"source_type\": doc.get(\"source_type\"),\n",
    "                        \"text\": chunk,  # <--- CRITICAL FOR RAG\n",
    "                        \"created_at\": doc.get(\"ingestion_timestamp\")\n",
    "                    })\n",
    "                    global_chunk_id += 1\n",
    "\n",
    "            logger.info(f\"Chunking complete. Generated {len(self.text_chunks)} chunks.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            raise KGException(e, sys)\n",
    "\n",
    "    def generate_embeddings(self):\n",
    "        \"\"\"\n",
    "        Step 2: Vectorization\n",
    "        Generates embeddings using the loaded model.\n",
    "        \"\"\"\n",
    "        logger.info(\"Starting embedding generation...\")\n",
    "        \n",
    "        try:\n",
    "            if not self.text_chunks:\n",
    "                logger.warning(\"No text chunks to embed.\")\n",
    "                return np.array([])\n",
    "\n",
    "            # Encode in batches to manage memory\n",
    "            batch_size = 32\n",
    "            embeddings = self.model.encode(\n",
    "                self.text_chunks,\n",
    "                batch_size=batch_size,\n",
    "                show_progress_bar=True,\n",
    "                convert_to_numpy=True,\n",
    "                normalize_embeddings=True # Good for cosine similarity search\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "            return embeddings\n",
    "\n",
    "        except Exception as e:\n",
    "            raise KGException(e, sys)\n",
    "\n",
    "    def save_vector_store(self, embeddings):\n",
    "        \"\"\"\n",
    "        Step 3: Storage (FAISS + Metadata)\n",
    "        \"\"\"\n",
    "        logger.info(\"Saving Vector Store and Metadata...\")\n",
    "        \n",
    "        try:\n",
    "            # --- A. Save Metadata ---\n",
    "            # We save metadata as a JSON list where index i corresponds to vector i\n",
    "            os.makedirs(os.path.dirname(self.metadata_path), exist_ok=True)\n",
    "            write_json(self.metadata_path, self.metadata)\n",
    "            \n",
    "            # --- B. Save FAISS Index ---\n",
    "            if len(embeddings) > 0:\n",
    "                dimension = embeddings.shape[1]\n",
    "                \n",
    "                # Using FlatL2 (Euclidean Distance). \n",
    "                # For huge datasets (>100k), consider IndexIVFFlat.\n",
    "                index = faiss.IndexFlatL2(dimension)\n",
    "                index.add(embeddings)\n",
    "                \n",
    "                os.makedirs(os.path.dirname(self.index_path), exist_ok=True)\n",
    "                faiss.write_index(index, self.index_path)\n",
    "                \n",
    "                logger.info(f\"FAISS index saved to {self.index_path}\")\n",
    "            else:\n",
    "                logger.warning(\"No embeddings to save.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            raise KGException(e, sys)\n",
    "\n",
    "    def run_pipeline(self):\n",
    "        \"\"\"\n",
    "        Main execution method\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.process_chunks()\n",
    "            vectors = self.generate_embeddings()\n",
    "            self.save_vector_store(vectors)\n",
    "            logger.info(\"Data Embedding Pipeline Completed Successfully.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise KGException(e, sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab864d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-19 19:03:48,242: INFO: common: YAML file: config\\config.yaml loaded successfully]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-19 19:03:48,451: INFO: SentenceTransformer: Use pytorch device_name: cpu]\n",
      "[2026-01-19 19:03:48,452: INFO: SentenceTransformer: Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2]\n",
      "[2026-01-19 19:03:54,656: INFO: 1370302600: Preparing text chunks]\n",
      "[2026-01-19 19:03:54,929: INFO: 1370302600: Total chunks created: 39565]\n",
      "[2026-01-19 19:03:54,930: INFO: 1370302600: Generating embeddings]\n",
      "Batches: 100%|██████████| 1237/1237 [01:11<00:00, 17.33it/s]\n",
      "[2026-01-19 19:05:06,591: INFO: 1370302600: Generation Completed]\n",
      "[2026-01-19 19:05:06,596: INFO: 1370302600: Storing embeddings in FAISS]\n",
      "[2026-01-19 19:05:06,688: INFO: 1370302600: FAISS index stored at: artifacts/embeddings/faiss.index]\n",
      "[2026-01-19 19:05:06,698: INFO: 1370302600: Storing embedding metadata]\n",
      "[2026-01-19 19:05:07,076: INFO: 1370302600: Metadata stored at: artifacts/embeddings/metadata.json]\n"
     ]
    }
   ],
   "source": [
    "config = ConfigurationManager()\n",
    "config = config.get_pipeline_embedd_config()\n",
    "obj = DataEmbedding(config)\n",
    "obj.prepare_chunks()\n",
    "vector = obj.generate_embeddings()\n",
    "obj.store_faiss_index(vector)\n",
    "obj.store_metadata()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb9fe355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vectors: 3221\n",
      "Vector dimension: 384\n"
     ]
    }
   ],
   "source": [
    "index = faiss.read_index(\"artifacts/embeddings/faiss.index\")\n",
    "print(\"Total vectors:\", index.ntotal)\n",
    "print(\"Vector dimension:\", index.d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09995a27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
