{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6c6a37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15df476d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\infosys\\\\notebook'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0263f001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\infosys'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f84171c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class DataIngestionConfig:\n",
    "    root_dir: str\n",
    "    email_dir: str\n",
    "    pdf_dir: str\n",
    "    csv_dir: str\n",
    "    db_path: str\n",
    "    output_json: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f830f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.knowledge_graph.utils.common import read_yaml\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(self, config_path=\"config/config.yaml\"):\n",
    "        self.config = read_yaml(config_path)\n",
    "\n",
    "    def get_ingestion_data_config(self) -> DataIngestionConfig:\n",
    "        di = self.config[\"ingestion_data\"]\n",
    "        return DataIngestionConfig(\n",
    "            root_dir=di[\"root_dir\"],\n",
    "            email_dir=di[\"email_dir\"],\n",
    "            pdf_dir=di[\"pdf_dir\"],\n",
    "            csv_dir=di[\"csv_dir\"],\n",
    "            db_path=di[\"db_path\"],\n",
    "            output_json=di[\"output_json\"],\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0c0259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from src.knowledge_graph.utils.common import write_json\n",
    "from src.knowledge_graph.logger.logging import logger\n",
    "from src.knowledge_graph.exception.exception import KGException\n",
    "import sys\n",
    "import pypdf # Requirement: pip install pypdf\n",
    "\n",
    "class DataIngestion:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.records = []\n",
    "        self.counter = 1\n",
    "\n",
    "    def _create_record(self, source_type, source_name, metadata, text):\n",
    "        \"\"\"Standardizes the record format.\"\"\"\n",
    "        # Skip empty text to reduce noise\n",
    "        if not text or not text.strip():\n",
    "            return None\n",
    "            \n",
    "        record = {\n",
    "            \"id\": self.counter,\n",
    "            \"source_type\": source_type,\n",
    "            \"source_name\": source_name,\n",
    "            \"metadata\": metadata,\n",
    "            \"text\": text.strip(),\n",
    "            \"ingestion_timestamp\": datetime.utcnow().isoformat()\n",
    "        }\n",
    "        self.counter += 1\n",
    "        return record\n",
    "\n",
    "    def _row_to_text(self, row, columns):\n",
    "        \"\"\"\n",
    "        Converts a dataframe row to a semantic string.\n",
    "        Before: \"John 30 Engineer\"\n",
    "        After: \"Name: John, Age: 30, Role: Engineer\"\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return \", \".join([f\"{col}: {val}\" for col, val in zip(columns, row) if pd.notna(val)])\n",
    "        except Exception:\n",
    "            return \" \".join(map(str, row))\n",
    "\n",
    "    # ---------- EMAIL INGESTION ----------\n",
    "    def ingest_emails(self):\n",
    "        logger.info(\"Starting Email Ingestion...\")\n",
    "        try:\n",
    "            files = [f for f in os.listdir(self.config.email_dir) if f.endswith(\".txt\")]\n",
    "            \n",
    "            for file in files:\n",
    "                path = os.path.join(self.config.email_dir, file)\n",
    "                try:\n",
    "                    with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "                        content = f.read()\n",
    "\n",
    "                    # Split header and body (assuming standard double newline separation)\n",
    "                    parts = content.split(\"\\n\\n\", 1)\n",
    "                    header_block = parts[0] if len(parts) > 0 else \"\"\n",
    "                    body_text = parts[1] if len(parts) > 1 else \"\"\n",
    "                    \n",
    "                    # Parse headers safely\n",
    "                    headers = {}\n",
    "                    for line in header_block.splitlines():\n",
    "                        if \":\" in line:\n",
    "                            key, value = line.split(\":\", 1)\n",
    "                            headers[key.strip().lower()] = value.strip()\n",
    "\n",
    "                    # Fallback if body is empty but content exists\n",
    "                    if not body_text and not headers:\n",
    "                        body_text = content\n",
    "\n",
    "                    record = self._create_record(\n",
    "                        source_type=\"email\",\n",
    "                        source_name=file,\n",
    "                        metadata={\n",
    "                            \"from\": headers.get(\"from\"),\n",
    "                            \"to\": headers.get(\"to\"),\n",
    "                            \"date\": headers.get(\"date\"),\n",
    "                            \"subject\": headers.get(\"subject\")\n",
    "                        },\n",
    "                        text=body_text\n",
    "                    )\n",
    "                    if record: self.records.append(record)\n",
    "\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to process email {file}: {e}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Critical error in email ingestion: {e}\")\n",
    "\n",
    "    # ---------- PDF INGESTION ----------\n",
    "    def ingest_pdfs(self):\n",
    "        logger.info(\"Starting PDF Ingestion...\")\n",
    "        try:\n",
    "            files = [f for f in os.listdir(self.config.pdf_dir) if f.endswith(\".pdf\")]\n",
    "            \n",
    "            for file in files:\n",
    "                path = os.path.join(self.config.pdf_dir, file)\n",
    "                try:\n",
    "                    text_content = []\n",
    "                    reader = pypdf.PdfReader(path)\n",
    "                    \n",
    "                    for page in reader.pages:\n",
    "                        extracted = page.extract_text()\n",
    "                        if extracted:\n",
    "                            text_content.append(extracted)\n",
    "                    \n",
    "                    full_text = \"\\n\".join(text_content)\n",
    "\n",
    "                    record = self._create_record(\n",
    "                        source_type=\"pdf\",\n",
    "                        source_name=file,\n",
    "                        metadata={\"pages\": len(reader.pages)},\n",
    "                        text=full_text\n",
    "                    )\n",
    "                    if record: self.records.append(record)\n",
    "\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to process PDF {file}: {e}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Critical error in PDF ingestion: {e}\")\n",
    "\n",
    "    # ---------- CSV INGESTION ----------\n",
    "    def ingest_csvs(self):\n",
    "        logger.info(\"Starting CSV Ingestion...\")\n",
    "        try:\n",
    "            files = [f for f in os.listdir(self.config.csv_dir) if f.endswith(\".csv\")]\n",
    "            \n",
    "            for file in files:\n",
    "                path = os.path.join(self.config.csv_dir, file)\n",
    "                try:\n",
    "                    # Use chunksize to handle large CSVs without memory crash\n",
    "                    chunk_iterator = pd.read_csv(path, chunksize=1000)\n",
    "                    \n",
    "                    for chunk in chunk_iterator:\n",
    "                        columns = list(chunk.columns)\n",
    "                        for row in chunk.values:\n",
    "                            text = self._row_to_text(row, columns)\n",
    "                            \n",
    "                            record = self._create_record(\n",
    "                                source_type=\"csv\",\n",
    "                                source_name=file,\n",
    "                                metadata={\"columns\": columns},\n",
    "                                text=text\n",
    "                            )\n",
    "                            if record: self.records.append(record)\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to process CSV {file}: {e}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Critical error in CSV ingestion: {e}\")\n",
    "\n",
    "    # ---------- DATABASE INGESTION ----------\n",
    "    def ingest_db(self):\n",
    "        logger.info(\"Starting Database Ingestion...\")\n",
    "        conn = None\n",
    "        try:\n",
    "            conn = sqlite3.connect(self.config.db_path)\n",
    "            cursor = conn.cursor()\n",
    "\n",
    "            # Get all tables\n",
    "            cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "            tables = cursor.fetchall()\n",
    "\n",
    "            for (table_name,) in tables:\n",
    "                try:\n",
    "                    # Read in chunks using pandas\n",
    "                    # 'chunksize' returns a generator of DataFrames\n",
    "                    for chunk in pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn, chunksize=1000):\n",
    "                        columns = list(chunk.columns)\n",
    "                        \n",
    "                        for row in chunk.values:\n",
    "                            text = self._row_to_text(row, columns)\n",
    "                            \n",
    "                            record = self._create_record(\n",
    "                                source_type=\"database\",\n",
    "                                source_name=table_name,\n",
    "                                metadata={\"columns\": columns, \"db_source\": self.config.db_path},\n",
    "                                text=text\n",
    "                            )\n",
    "                            if record: self.records.append(record)\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to ingest table {table_name}: {e}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Database connection error: {e}\")\n",
    "        finally:\n",
    "            if conn: conn.close()\n",
    "\n",
    "    # ---------- MAIN PIPELINE ----------\n",
    "    def ingest(self):\n",
    "        try:\n",
    "            logger.info(f\">>> Ingestion Started at {datetime.now()}\")\n",
    "            \n",
    "            self.ingest_emails()\n",
    "            self.ingest_pdfs()\n",
    "            self.ingest_csvs()\n",
    "            self.ingest_db()\n",
    "\n",
    "            write_json(self.config.output_json, self.records)\n",
    "            \n",
    "            logger.info(f\"<<< Ingestion Completed. Total Records: {len(self.records)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise KGException(e, sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdb2565",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ConfigurationManager()\n",
    "di_config = config.get_ingestion_data_config()\n",
    "ingestion = DataIngestion(di_config)\n",
    "ingestion.ingest()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
